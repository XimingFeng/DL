

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>15.4 Distributed Representation &mdash; dl 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Extra" href="../../Extra/index.html" />
    <link rel="prev" title="15.3 Semi-Supervised Disentangling of Casual Factors" href="15.3 Semi Supervised Disentangling of Casual Factors.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> dl
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../Part 1 (Applied Math and Machine Learning Basics)/index.html">Part I: Applied Math and Machine Learning Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Part 2 (Modern Practical Deep Networks)/index.html">Part II: Modern Practical Deep Networks</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Part III: Deep Learning Research</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../14 Autoencoders/index.html">14 Autoencoders</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">15 Representation Learning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="15.1 Gready Layer Wise Unsupervised Pretraining.html">15.1 Gready Layer-Wise Unsupervised Pretraining</a></li>
<li class="toctree-l3"><a class="reference internal" href="15.2 Transfer Learning and Domain Adaptation.html">15.2 Transfer Learning and Domain Adaptation</a></li>
<li class="toctree-l3"><a class="reference internal" href="15.3 Semi Supervised Disentangling of Casual Factors.html">15.3 Semi-Supervised Disentangling of Casual Factors</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">15.4 Distributed Representation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#review-on-knn-5-7-3">Review on KNN 5.7.3</a></li>
<li class="toctree-l4"><a class="reference internal" href="#review-on-decision-tree-5-7-3">Review on decision tree 5.7.3</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resources">Resources</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Extra/index.html">Extra</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">dl</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Part III: Deep Learning Research</a> &raquo;</li>
        
          <li><a href="index.html">15 Representation Learning</a> &raquo;</li>
        
      <li>15.4 Distributed Representation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/Part 3 (Deep Learning Research)/15 Representation Learning/15.4 Distributed Representation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="distributed-representation">
<h1>15.4 Distributed Representation<a class="headerlink" href="#distributed-representation" title="Permalink to this headline">¶</a></h1>
<ul>
<li><p class="first">Distributed representations: representations composed of many elements that can be set seperately from each other.</p>
</li>
<li><p class="first">Symbolic representation: the input is associated with <strong>a single</strong> simbol or a category.</p>
<blockquote>
<div><ul class="simple">
<li>If there are n symbols in the dictionary, one can imagine n feature detctors, each corresponding to the detection of the presence of the associated category.</li>
<li>Only n different configurations of the representation space are possible, carving n different regions in input space.</li>
<li>Also called one-hot representation</li>
<li>One example of non-distributed representation.</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>Nondistributed representations: representations that may contain many entries but without significant meaningful seperate control over each entry.</p>
<p>Examples of learning algo based on nondistributed representation learning:</p>
<ul class="simple">
<li>Clustering method, include the k-means algorithm: each input assigned to exactly one cluster</li>
<li><a class="reference external" href="https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26">K Nearest neighbor</a>: If k &gt; 1, multiple values describe each input, but they cannot be controlled seperately from each other, so this does not qualify as a true distributed representation.</li>
<li>Decision tree: Only one leaf is activated when the input is given</li>
<li>Gaussian mixture and mixtures of expert: each input is represented with multiple values, but those values cannot be readily be controlled seperatly from each other.</li>
<li>Kernel machine with a Gausian kernel</li>
<li>Language or translation models based on g-grams</li>
</ul>
<p>See the review for some of the ML algorithms at the end of this summary E.g. of 3 binary features representation:</p>
<img alt="../../_images/Figure15.7.PNG" src="../../_images/Figure15.7.PNG" />
<p>E.g. of non-distributed representation: nearest neighbor</p>
<img alt="../../_images/Figure15.8.PNG" src="../../_images/Figure15.8.PNG" />
<p>One important concept in distributed representation: generalization arised due to shared attributes between different concepts. Distributed representation include a rich similarity space, in which semantically close concepts (or inputs) are close in distance. This is a property absnet from purely symbolic representation.</p>
<p>Distributed representation can have statistical advantages when an apparently complecated structure can be compactly represented using a small number parameters.</p>
<p>E.g. See figure 15.7. Each binary feature binary feature in this representation divides <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> into a pair of half spaces. The exponentially large number of intersection of n of correcponding half spaces determines how many regions this distributed representation learner can distinguish. The number of regions this binary feature representation can distinguish is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\sum ^d_{j=0} \begin{pmatrix} n \\ j \end{pmatrix} = O(n^d)\end{split}\]</div>
<p>We can see a growth that is exponential in the input size and polynomial in the number of hidden units.</p>
<p>With O(nd) parameters (for n linear threshold features in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>), we can distinctly represent <span class="math notranslate nohighlight">\(O(n^d)\)</span> region in the input space. If we have assumption at all about the data, and used a representation with one unique symbol for each region and seperate parameters for each symbol to recognize its corresponding portion of <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>, the specify <span class="math notranslate nohighlight">\(O(n^d)\)</span> region would require <span class="math notranslate nohighlight">\(O(n^d)\)</span> examples.</p>
<p>If a parametric transformation with k parameters can learn about r regions in input space, with k &lt;&lt; k, and if obtaining such a representation was useful to the task of interest, then we could potentially generalize much better in this way than in a nondistributed settings, where we would need O(r) exmaples to obtain the same features and associated partitioning of the input space into r regions.</p>
<p>Another reason why distributed representation generalize well: their capacity remains limited despite being able to distincly encode so many different regions.
Reason of limited capacity: While we can assign very many unique codes to representation space, we cannot use absolutely all the code space, nor can we learn arbitrary functions mapping from the representation space h to the outputs space y using a linear classfier. The use of distributed representation combined with a linear classifier thus express a prior belief that the classes to recognized are linearly seperable as a function of the underlying causal factors captured by h. e.g we don’t want to partition data into the set of all red cars and green trucks as one class and the set of all green cars and red trucks as another class.</p>
<p>Valdation of distributed representation:</p>
<img alt="../../_images/Figure15.9.PNG" src="../../_images/Figure15.9.PNG" />
<div class="section" id="review-on-knn-5-7-3">
<h2>Review on KNN 5.7.3<a class="headerlink" href="#review-on-knn-5-7-3" title="Permalink to this headline">¶</a></h2>
<p>One weakness of k-nearest neighbors is that it cannot learn that one feature is more discriminative than another. For example,imagine we have a regression task withx ∈ R 100 drawn from an isotropic Gaussian distribution, but only a single variable <span class="math notranslate nohighlight">\(x_1\)</span> is relevant to the output. Suppose further that this feature simply encodes the output directly, that <span class="math notranslate nohighlight">\(y=x_1\)</span> in all cases. Nearest neighbor regression will not be able to detect this simple pattern.The nearest neighbor of most pointsx will be determined by the large number of features <span class="math notranslate nohighlight">\(x_2\)</span> through <span class="math notranslate nohighlight">\(x_{100}\)</span>, not by the lone feature <span class="math notranslate nohighlight">\(x_1\)</span>. Thus the output on small training sets will essentially be random.</p>
</div>
<div class="section" id="review-on-decision-tree-5-7-3">
<h2>Review on decision tree 5.7.3<a class="headerlink" href="#review-on-decision-tree-5-7-3" title="Permalink to this headline">¶</a></h2>
<img alt="../../_images/Figure5.7.PNG" src="../../_images/Figure5.7.PNG" />
</div>
<div class="section" id="resources">
<h2>Resources<a class="headerlink" href="#resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26">K Nearest neighbor</a></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Extra/index.html" class="btn btn-neutral float-right" title="Extra" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="15.3 Semi Supervised Disentangling of Casual Factors.html" class="btn btn-neutral" title="15.3 Semi-Supervised Disentangling of Casual Factors" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Ximing

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>